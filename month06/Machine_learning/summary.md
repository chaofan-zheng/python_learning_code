# 机器学习

## 一、基本概念

### 1. 定义

如果一个系统，通过执行某个过程，改善了它的性能，这个过程叫学习

### 2. 机器学习分类

1）有监督、无监督、半监督

- 有监督：样本经过标注，包含输入、输出，包括回归、分类
- 无监督：样本没有经过标注，只包含输入，不包含输出，如：聚类
- 半监督：有监督、无监督学习结合

2）批量学习、增量学习

- 批量学习：学习完成，再使用
- 增量学习：边学习、边使用

3）基于模型、基于实例的学习

- 基于模型的学习：从数据中找规律（数学模型），如线性回归、多项式回归、朴素贝叶斯、SVM
- 基于实例的学习：从数据中找答案，如决策树

### 3. 机器学习的基本问题

1）回归问题：预测结果为连续值

2）分类问题：预测结果为离散值

3）聚类问题：无监督学习，根据样本相似度，将相似度较高的划分到同一个聚簇中

4）降维问题：降低数据或模型的复杂度

### 4. 机器学习的一般过程

准备数据 --> 数据清洗 --> 选择模型 --> 训练 --> 评估 --> 测试 --> 应用及维护

## 二、数据预处理

1）标准化（均值移除）：将每列数据转换为均值为0、标准差为1的分布

2）范围缩放：将每列最小值转换为0，最大值转换为1

3）归一化：将每行的数值转换为百分比

4）二值化：将数值转换为0/1中的一个

5）独热编码：将数值转换为一个1和一串0表示的格式

6）标签编码：将字符串转换为数值

## 三、回归问题

### 1. 线性回归

1）线性模型： $y = w^Tx + b$ (自变量没有高次项)

2）线性回归：根据样本的分布（基本呈线性分布），寻找一个最优的线性模型，使用该模型执行预测

3）损失函数与梯度下降

- 损失函数：度量预测值和真实值之间的差异，回归问题采用均方差损失函数

$$
E = \frac{1}{2} \sum_i^N(y - y')^2
$$

- 梯度下降

  - 什么是梯度下降：沿损失函数梯度负方向寻找损失函数最小值（替代寻找最优参数）
  - 为什么用梯度下降：工程上直接进行求解比较困难

  $$
  w_i = w_i + \Delta w_i \\
  \Delta w_i = - \eta \frac{\partial E}{\partial w_i}
  $$

- 线性回归变种

  - Lasso回归：在标准线性回归损失函数上添加了L1范数
  - 岭回归：在标准线性回归损失函数上添加了L2范数

### 2. 多项式回归

1）多项式：引入高次项，用于样本呈非线性分布的情况

2）过拟合、欠拟合

- 过拟合：模型过分拟合于训练样本，导致泛化能力不足，典型表现为训练集下准确率较高，测试集下准确率较低
- 欠拟合：拟合程度不够，模型没有学习到数据规律，表现为训练集、测试集下准确率都不高
- 处理方式
  - 过拟合：降低模型复杂度、减少特征数量
  - 欠拟合：增加模型复杂度、增加特征数量

### 3. 决策树回归

1）定义：利用树状结构，对样本的属性进行判断，将具有相同属性的样本划分到同一个子节点下，根据叶子节点下的样本求均值实现回归、投票法实现分类

2）结构：根节点、中间节点（判断节点）、叶子节点（样本）

3）信息熵：度量一个系统（一批数据）混乱/有序的指标，熵越大说明系统越混乱/无序，熵越小说明系统越纯净/有序

4）决策树选择最优属性的依据

- 信息增益：划分前、划分后信息熵差值
- 增益率：信息增益/固有熵
- 基尼系数

5）决策树枝剪：前枝剪、后枝剪

6）集成学习

- 定义：多个模型学习，克服单个模型的局限
- 两种集成方式
  - Boosting：模型之间强关联，如AdaBoosting
  - Bagging：模型之间弱关联，如专门为决策树设计的随机森林

## 四、分类问题

### 1. 定义

预测结果是离散的称为分类问题

### 2. 逻辑回归

1）定义：二分类问题，先利用回归模型计算出一个连续值，再利用逻辑函数进行离散化

2）Sigmoid(逻辑函数)：将负无穷到正无穷范围的数字转换到0~1之间
$$
y = \frac{1}{1+e^{-t}}
$$
3）损失函数：分类问题使用交叉熵作为损失函数

4）多分类：可以利用多个二分类模型实现多分类

### 3. 决策树

略

### 4. 支持向量机

1）定义：二分类模型，在样本间寻找一个线性模型作为分类边界（称为分割超平面），使得边界离支持向量距离最大化

2）分类边界要求：正确性、公平性、安全性、简单性

3）线性可分、线性不可分

4）核函数：将线性不可分问题转换为高纬度空间下的线性可分问题

- 线性核函数：不升维，在原始空间下寻找分类边界，主要用于线性可分问题
- 多项式核函数：多项式升维
- 径项基核函数：高斯核函数升维

### 5. 朴素贝叶斯

1）关于概率的术语

- 随机事件：可能出现、可能不出现的事件
- 概率：随机事件出现的可能性大小，记作P(A)
- 联合概率：多个事件同事出现的概率，P(A,B)
- 条件概率：给定某个条件下，随机事件出现的概率，记作P(A|B)
- 先验概率：没有给定任何信息情况下得到的概率
- 后验概率：给定任何信息情况下得到的概率

2）贝叶斯定理
$$
P(A|B) = \frac{P(A)P(B|A)}{P(B)}
$$
3）朴素贝叶斯：朴素是指假设事件是独立出现，利用贝叶斯定理计算给定数据情况下属于某个类别的概率

## 五、聚类

1）定义：无监督学习，根据样本的相似度，将其划分到不同的群落（聚簇）

2）相似度度量

- 欧氏距离
- 曼哈顿距离
- 切比雪夫距离
- 闵可夫斯基距离：前三种距离都是该距离的特殊形式

3）聚类的划分

- 基于原型的聚类：如k-means
- 基于密度的聚类：如DBSCAN
- 基于层次的聚类：如凝聚层次

4）常用的聚类算法

| 比较项               | K-Means  | DBSCAN   | 凝聚层次 |
| -------------------- | -------- | -------- | -------- |
| 类别                 | 基于原型 | 基于密度 | 基于层次 |
| 有没有聚类中心       | 有       | 无       | 无       |
| 是否需要知道聚类数量 | 是       | 否       | 是       |
| 噪声样本是否敏感     | 敏感     | 不敏感   | 不敏感   |

5）评价指标：轮廓系数

## 六、模型评估优化

1）回归问题评价

- 均方差（用作损失函数）
- R2值

2）分类问题评价

- 准确率（Accuracy）：预测正确样本数量 / 样本总数
- 错误率（Error Rate）：预测错误样本数量 / 样本总数
- 查准率（Precision）：分得准不准，TP / TP + FP
- 召回率（Recall）：分得全不全（又叫查全率）, TP / TP + FN

3）混淆矩阵

4）交叉验证法：将数据集划分成k个子集（每个子集称为一个折叠），每次训练使用其中一个折叠作为测试集，其它折叠作为训练集，主要用于样本较少的情况下模型评估

5）验证曲线、学习曲线

- 验证曲线：比较不同的参数对模型的影响
- 学习曲线：比较不同规模的训练集对模型的影响

6）超参数的优化

- 超参数：提前设置的参数，不是通过模型的学习得来的
  - 学习率
  - 决策树的深度
  - 随机森林树的棵数
  - 正则强度
  - 交叉验证折叠数量
  - 训练集、测试集的比率
  - 正态分布函数的标准差
- 网格搜索、随机搜索





